# Chapter 23 학습 노트 - 활성화 함수(ReLU/Sigmoid/Tanh) 비교

## 학습 목표
- 활성화 함수가 출력 범위와 기울기에 주는 영향을 실습합니다.
- chapter23/practice.py를 실행해 핵심 값을 직접 확인합니다.

## 핵심 개념
- 이 챕터는 chapter21의 내용을 세분화한 파트입니다.
- 수식을 코드로 옮길 때, `입력 shape -> 가중치 shape -> 출력 shape`를 항상 먼저 점검합니다.

## 실행 방법
```bash
cd chapters/chapter23
python practice.py
```

## 체크 포인트
1. 출력 딕셔너리의 `chapter`, `topic` 필드 확인
2. 각 챕터별 핵심 지표(손실, 정확도, shape, 확률 등) 확인
3. 실험 값(학습률, 에폭, 초기화 스케일)을 바꿔 추가 실험

## 확장 아이디어
- numpy만으로 구현한 연산을 PyTorch/TensorFlow와 비교하기
- 데이터 샘플 수를 늘려 일반화 성능 변화를 확인하기
